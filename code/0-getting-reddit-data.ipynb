{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "The problem statement and analysis of the data collected here are found in the [second notebook](https://github.com/q-downgrade/language-trans-queer-communities-nlp/blob/master/code/1-modeling-and-analyzing-data.ipynb) of this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import time\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting off by establishing the base url for the Reddit API I'm using and the two subreddits I'll be gathering data from, just to make it easier. For this project I'll use r/traaaaaaannnnnnnnnns, a popular subreddit for trans people, and r/actuallesbians, a popular subreddit for queer women no matter how they identify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sstablish URL base\n",
    "base_url = \"https://api.pushshift.io/reddit/search/submission\" # also known as the \"API endpoint\" \n",
    "\n",
    "# establish parameters\n",
    "r_traa = \"traaaaaaannnnnnnnnns\"\n",
    "r_lesbian = 'actuallesbians'\n",
    "size = 500\n",
    "\n",
    "# construct full url\n",
    "stem_traa = f\"{base_url}?subreddit={r_traa}&size={size}\"\n",
    "stem_les = f\"{base_url}?subreddit={r_lesbian}&size={size}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(subreddit, stem):\n",
    "    '''\n",
    "    Function takes in a subreddit and a url stem and uses them to get data through the Pushshift API.\n",
    "    '''\n",
    "    # loop variables\n",
    "    day_window = 5 \n",
    "    n = 8\n",
    "\n",
    "    # establish empty list\n",
    "    posts_subreddit = []\n",
    "\n",
    "    # loop from 1 until n + 1\n",
    "    for i in range(1, n + 1):\n",
    "        # create custom URL \n",
    "        URL = f\"{stem}&after={i * day_window}d\"\n",
    "        print(\"Querying from: \" + URL)\n",
    "        res = requests.get(URL)\n",
    "        assert res.status_code == 200\n",
    "        json = res.json()['data']\n",
    "        df = pd.DataFrame(json)\n",
    "        posts_subreddit.append(df)\n",
    "        time.sleep(2)\n",
    "\n",
    "    print(\"Query complete!\")\n",
    "    return posts_subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying from: https://api.pushshift.io/reddit/search/submission?subreddit=traaaaaaannnnnnnnnns&size=500&after=5d\n",
      "Querying from: https://api.pushshift.io/reddit/search/submission?subreddit=traaaaaaannnnnnnnnns&size=500&after=10d\n",
      "Querying from: https://api.pushshift.io/reddit/search/submission?subreddit=traaaaaaannnnnnnnnns&size=500&after=15d\n",
      "Querying from: https://api.pushshift.io/reddit/search/submission?subreddit=traaaaaaannnnnnnnnns&size=500&after=20d\n",
      "Querying from: https://api.pushshift.io/reddit/search/submission?subreddit=traaaaaaannnnnnnnnns&size=500&after=25d\n",
      "Querying from: https://api.pushshift.io/reddit/search/submission?subreddit=traaaaaaannnnnnnnnns&size=500&after=30d\n",
      "Querying from: https://api.pushshift.io/reddit/search/submission?subreddit=traaaaaaannnnnnnnnns&size=500&after=35d\n",
      "Querying from: https://api.pushshift.io/reddit/search/submission?subreddit=traaaaaaannnnnnnnnns&size=500&after=40d\n",
      "Query complete!\n"
     ]
    }
   ],
   "source": [
    "posts_traa = get_data(r_traa, stem_traa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting data from r/traaaaaaannnnnnnnnns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying from: https://api.pushshift.io/reddit/search/submission?subreddit=actuallesbians&size=500&after=5d\n",
      "Querying from: https://api.pushshift.io/reddit/search/submission?subreddit=actuallesbians&size=500&after=10d\n",
      "Querying from: https://api.pushshift.io/reddit/search/submission?subreddit=actuallesbians&size=500&after=15d\n",
      "Querying from: https://api.pushshift.io/reddit/search/submission?subreddit=actuallesbians&size=500&after=20d\n",
      "Querying from: https://api.pushshift.io/reddit/search/submission?subreddit=actuallesbians&size=500&after=25d\n",
      "Querying from: https://api.pushshift.io/reddit/search/submission?subreddit=actuallesbians&size=500&after=30d\n",
      "Querying from: https://api.pushshift.io/reddit/search/submission?subreddit=actuallesbians&size=500&after=35d\n",
      "Querying from: https://api.pushshift.io/reddit/search/submission?subreddit=actuallesbians&size=500&after=40d\n",
      "Query complete!\n"
     ]
    }
   ],
   "source": [
    "posts_les = get_data(r_lesbian, stem_les)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting data from r/actuallesbians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_traa = pd.concat(posts_traa, sort = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_les = pd.concat(posts_les, sort = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turning both lists into dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 75)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_traa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lindsayleedham/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df_final = pd.concat([df_traa, df_les])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining the two dataframes into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 80)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['all_awardings', 'allow_live_comments', 'author', 'author_cakeday',\n",
       "       'author_flair_background_color', 'author_flair_css_class',\n",
       "       'author_flair_richtext', 'author_flair_template_id',\n",
       "       'author_flair_text', 'author_flair_text_color', 'author_flair_type',\n",
       "       'author_fullname', 'author_patreon_flair', 'author_premium', 'awarders',\n",
       "       'banned_by', 'can_mod_post', 'contest_mode', 'created_utc',\n",
       "       'crosspost_parent', 'crosspost_parent_list', 'distinguished', 'domain',\n",
       "       'edited', 'full_link', 'gilded', 'gildings', 'id', 'is_crosspostable',\n",
       "       'is_meta', 'is_original_content', 'is_reddit_media_domain',\n",
       "       'is_robot_indexable', 'is_self', 'is_video',\n",
       "       'link_flair_background_color', 'link_flair_css_class',\n",
       "       'link_flair_richtext', 'link_flair_template_id', 'link_flair_text',\n",
       "       'link_flair_text_color', 'link_flair_type', 'locked', 'media',\n",
       "       'media_embed', 'media_metadata', 'media_only', 'no_follow',\n",
       "       'num_comments', 'num_crossposts', 'over_18', 'parent_whitelist_status',\n",
       "       'permalink', 'pinned', 'post_hint', 'preview', 'pwls',\n",
       "       'removed_by_category', 'retrieved_on', 'score', 'secure_media',\n",
       "       'secure_media_embed', 'selftext', 'send_replies', 'spoiler',\n",
       "       'steward_reports', 'stickied', 'subreddit', 'subreddit_id',\n",
       "       'subreddit_subscribers', 'subreddit_type', 'suggested_sort',\n",
       "       'thumbnail', 'thumbnail_height', 'thumbnail_width', 'title',\n",
       "       'total_awards_received', 'url', 'whitelist_status', 'wls'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the list of columns in the dataframe. Many of these are Reddit-specific, such as \"author_cakeday\" which lists the day the person signed up for Reddit. We can drop many of these from the final dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['title', 'selftext', 'subreddit', 'created_utc', 'author', \n",
    "        'author_flair_text', 'score', 'is_self']\n",
    "df_final = df_final[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keeping the title and selftext (the body of the posts, if it's text) for analyzing in the next steps. Also including author_flair_text in case that proves helpful or interesting later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resetting the index of the final dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2020-01-21\n",
       "1    2020-01-21\n",
       "2    2020-01-21\n",
       "3    2020-01-21\n",
       "4    2020-01-21\n",
       "Name: timestamp, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating `timestamp` column using `created_utc` column\n",
    "df_final[\"timestamp\"] = df_final['created_utc'].map(dt.date.fromtimestamp)\n",
    "df_final['timestamp'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>author</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>score</th>\n",
       "      <th>is_self</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>We Vibin</td>\n",
       "      <td></td>\n",
       "      <td>traaaaaaannnnnnnnnns</td>\n",
       "      <td>1579648948</td>\n",
       "      <td>RemIsAMess</td>\n",
       "      <td>Remy|FtM|Peecock Gen 4</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-01-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>First week of T be like</td>\n",
       "      <td></td>\n",
       "      <td>traaaaaaannnnnnnnnns</td>\n",
       "      <td>1579649007</td>\n",
       "      <td>bubblegumblueart</td>\n",
       "      <td>Spicy FTMemer</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-01-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>When you put on breast forms for the first time</td>\n",
       "      <td></td>\n",
       "      <td>traaaaaaannnnnnnnnns</td>\n",
       "      <td>1579649035</td>\n",
       "      <td>craft6886</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-01-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>please, I need to know</td>\n",
       "      <td></td>\n",
       "      <td>traaaaaaannnnnnnnnns</td>\n",
       "      <td>1579649862</td>\n",
       "      <td>sociopathic_muffin</td>\n",
       "      <td>cant get a bf, became the bf</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-01-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>\"I don't have E\" is such a h*cking mood</td>\n",
       "      <td></td>\n",
       "      <td>traaaaaaannnnnnnnnns</td>\n",
       "      <td>1579650222</td>\n",
       "      <td>A_Wild_Rat_Appeared</td>\n",
       "      <td>mtr, Male To Rat.</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-01-21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                            title selftext  \\\n",
       "0      0                                         We Vibin            \n",
       "1      1                          First week of T be like            \n",
       "2      2  When you put on breast forms for the first time            \n",
       "3      3                           please, I need to know            \n",
       "4      4          \"I don't have E\" is such a h*cking mood            \n",
       "\n",
       "              subreddit  created_utc               author  \\\n",
       "0  traaaaaaannnnnnnnnns   1579648948           RemIsAMess   \n",
       "1  traaaaaaannnnnnnnnns   1579649007     bubblegumblueart   \n",
       "2  traaaaaaannnnnnnnnns   1579649035            craft6886   \n",
       "3  traaaaaaannnnnnnnnns   1579649862   sociopathic_muffin   \n",
       "4  traaaaaaannnnnnnnnns   1579650222  A_Wild_Rat_Appeared   \n",
       "\n",
       "              author_flair_text  score  is_self   timestamp  \n",
       "0        Remy|FtM|Peecock Gen 4      1    False  2020-01-21  \n",
       "1                 Spicy FTMemer      1    False  2020-01-21  \n",
       "2                          None      1    False  2020-01-21  \n",
       "3  cant get a bf, became the bf      1    False  2020-01-21  \n",
       "4             mtr, Male To Rat.      1    False  2020-01-21  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.drop(columns = 'index', inplace = True)\n",
    "df_final.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_final.to_csv('../datasets/data.csv', index = False)\n",
    "#commenting out to prevent overwriting the data use by the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** All data was obtained on 1/26/20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
